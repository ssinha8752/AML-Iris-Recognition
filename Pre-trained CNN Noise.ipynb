{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","toc_visible":true,"authorship_tag":"ABX9TyP0JYKe+Q1WfVjAb1YHIxzM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8IYxgWuNDbz2","executionInfo":{"status":"ok","timestamp":1751723717125,"user_tz":-480,"elapsed":25986,"user":{"displayName":"liu muzi","userId":"17190698434235273658"}},"outputId":"6dbfb2d2-ba05-4460-9643-b5418c8f1d1d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# prompt: mount drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torchvision import models, transforms\n","from torch.utils.data import DataLoader, Dataset\n","from PIL import Image\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder, Normalizer\n","from sklearn.svm import SVC\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","import torch.optim as optim\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","from datetime import datetime\n","import os\n","import joblib\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","\n","import torchvision.transforms as T\n","from torchvision.models import resnet18, ResNet18_Weights\n","\n","import torchvision.models as models\n","from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights"],"metadata":{"id":"ZFmTDvSiFDTi","executionInfo":{"status":"ok","timestamp":1751724985459,"user_tz":-480,"elapsed":10370,"user":{"displayName":"liu muzi","userId":"17190698434235273658"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# 1. Load CSV\n","df = pd.read_csv('/content/drive/MyDrive/AML-PROJECT/623final_all.csv')\n","\n","\n","# 2. Extract ID\n","df['ID'] = df['label'].str.extract(r'(\\d+)', expand=False)\n","\n","# Optional: Save filtered train/val\n","train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['ID'], random_state=42)\n","\n","# 3. Encode labels\n","le = LabelEncoder()\n","train_df['encoded_label'] = le.fit_transform(train_df['label'])\n","val_df['encoded_label'] = le.transform(val_df['label'])"],"metadata":{"id":"4TQiUL8LkJ8c","executionInfo":{"status":"ok","timestamp":1751725109865,"user_tz":-480,"elapsed":2552,"user":{"displayName":"liu muzi","userId":"17190698434235273658"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["### Resnet18 on Noise Dataset"],"metadata":{"id":"cb1TKhMvzr9d"}},{"cell_type":"code","source":["# 4. Dataset class\n","class IrisDataset(Dataset):\n","    def __init__(self, df, transform=None):\n","        self.df = df\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.df.iloc[idx]['image_path']\n","        label = self.df.iloc[idx]['encoded_label']\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label\n","\n","# 5.Define transformations and loaders\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406],\n","                         [0.229, 0.224, 0.225])\n","])\n","\n","train_dataset = IrisDataset(train_df, transform=transform)\n","val_dataset = IrisDataset(val_df, transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","\n","\n","# 6. Load ResNet18 and modify final layer\n","model = models.resnet18(pretrained=True)\n","num_classes = len(le.classes_)\n","model.fc = nn.Linear(model.fc.in_features, num_classes)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","\n","# 7.Loss function\n","# Define Focal Loss\n","class FocalLoss(nn.Module):\n","    def __init__(self, gamma=2.0):\n","        super(FocalLoss, self).__init__()\n","        self.gamma = gamma\n","        self.ce = nn.CrossEntropyLoss(reduction='none')\n","\n","    def forward(self, input, target):\n","        ce_loss = self.ce(input, target)\n","        pt = torch.exp(-ce_loss)\n","        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n","        return focal_loss.mean()\n","\n","# Define Label Smoothing Loss\n","class LabelSmoothingLoss(nn.Module):\n","    def __init__(self, classes, smoothing=0.1):\n","        super(LabelSmoothingLoss, self).__init__()\n","        self.confidence = 1.0 - smoothing\n","        self.smoothing = smoothing\n","        self.cls = classes\n","\n","    def forward(self, pred, target):\n","        pred = pred.log_softmax(dim=-1)\n","        with torch.no_grad():\n","            true_dist = torch.zeros_like(pred)\n","            true_dist.fill_(self.smoothing / (self.cls - 1))\n","            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n","        return torch.mean(torch.sum(-true_dist * pred, dim=-1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nj4-VL-UZcr3","executionInfo":{"status":"ok","timestamp":1751725110927,"user_tz":-480,"elapsed":936,"user":{"displayName":"liu muzi","userId":"17190698434235273658"}},"outputId":"cbe57e7f-b1cc-4657-9af8-f680b84eda12"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 193MB/s]\n"]}]},{"cell_type":"code","source":["# 8. Evaluation Function\n","def evaluate(model, loader, device):\n","    model.eval()\n","    y_true, y_pred = [], []\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x, y = x.to(device), y.to(device)\n","            outputs = model(x)\n","            preds = outputs.argmax(dim=1)\n","            y_true.extend(y.cpu().numpy())\n","            y_pred.extend(preds.cpu().numpy())\n","    return {\n","        'accuracy': accuracy_score(y_true, y_pred),\n","        'f1': f1_score(y_true, y_pred, average='weighted',zero_division=0),\n","        'precision': precision_score(y_true, y_pred, average='weighted',zero_division=0),\n","        'recall': recall_score(y_true, y_pred, average='weighted',zero_division=0)\n","    }\n","\n","# 9. Training Function\n","def train_resnet18(loss_fn, optimizer_cls, epochs=5):\n","    model = models.resnet18(pretrained=True)\n","    model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    model = model.to(device)\n","\n","    optimizer = optimizer_cls(model.parameters(), lr=1e-3)\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        total_loss = 0\n","        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n","            x, y = x.to(device), y.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(x)\n","            loss = loss_fn(outputs, y)\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","\n","        metrics = evaluate(model, val_loader, device)\n","        print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | \"\n","              f\"Acc: {metrics['accuracy']:.4f} | F1: {metrics['f1']:.4f}\")\n","\n","    final_metrics = evaluate(model, val_loader, device)\n","\n","\n","    timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n","    model_name = f\"resnet18__{loss_fn.__class__.__name__}__{optimizer_cls.__name__}__{timestamp}.pth\"\n","\n","    save_dir = \"/content/drive/MyDrive/AML-PROJECT/pretrained_cnn_Ly/model2\"\n","    os.makedirs(save_dir, exist_ok=True)\n","    model_path = os.path.join(save_dir, model_name)\n","\n","    torch.save(model.state_dict(), model_path)\n","    print(f\"Saved model to: {model_path}\")\n","\n","\n","    return {\n","        'loss_fn': loss_fn.__class__.__name__,\n","        'optimizer': optimizer_cls.__name__ if hasattr(optimizer_cls, '__name__') else str(optimizer_cls),\n","        'accuracy': final_metrics['accuracy'],\n","        'f1': final_metrics['f1'],\n","        'precision': final_metrics['precision'],\n","        'recall': final_metrics['recall'],\n","        'timestamp': datetime.now().isoformat()\n","    }"],"metadata":{"id":"u7462JSBFZoc","executionInfo":{"status":"ok","timestamp":1751725118596,"user_tz":-480,"elapsed":7,"user":{"displayName":"liu muzi","userId":"17190698434235273658"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# 10. Run Experiments\n","loss_fns = [\n","    nn.CrossEntropyLoss(),\n","    LabelSmoothingLoss(classes=num_classes),\n","    FocalLoss(gamma=2)\n","]\n","\n","optimizers = [optim.Adam, optim.SGD]\n","\n","results = []\n","\n","for loss_fn in loss_fns:\n","    for opt in optimizers:\n","        try:\n","            print(f\"\\nRunning: {loss_fn.__class__.__name__} + {opt.__name__}\")\n","            result = train_resnet18(loss_fn, opt, epochs=5)\n","            results.append(result)\n","        except Exception as e:\n","            print(f\"[ERROR] Skipped: {loss_fn}, {opt} — {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xYI4Zy0lFojG","executionInfo":{"status":"ok","timestamp":1751726830188,"user_tz":-480,"elapsed":1707639,"user":{"displayName":"liu muzi","userId":"17190698434235273658"}},"outputId":"91e58f81-f75c-42c0-d8c6-0c446bbce5dd"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Running: CrossEntropyLoss + Adam\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/5: 100%|██████████| 50/50 [19:25<00:00, 23.30s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 | Loss: 5.2454 | Acc: 0.0575 | F1: 0.0347\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/5: 100%|██████████| 50/50 [00:07<00:00,  6.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2 | Loss: 3.1189 | Acc: 0.2975 | F1: 0.2485\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/5: 100%|██████████| 50/50 [00:07<00:00,  7.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3 | Loss: 1.3941 | Acc: 0.5900 | F1: 0.5514\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/5: 100%|██████████| 50/50 [00:07<00:00,  7.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4 | Loss: 0.5157 | Acc: 0.6575 | F1: 0.6349\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/5: 100%|██████████| 50/50 [00:06<00:00,  7.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5 | Loss: 0.1870 | Acc: 0.8375 | F1: 0.8246\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Saved model to: /content/drive/MyDrive/AML-PROJECT/pretrained_cnn_Ly/model2/resnet18__CrossEntropyLoss__Adam__20250705-144325.pth\n","\n","Running: CrossEntropyLoss + SGD\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/5: 100%|██████████| 50/50 [00:06<00:00,  7.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 | Loss: 5.4799 | Acc: 0.0025 | F1: 0.0037\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/5: 100%|██████████| 50/50 [00:07<00:00,  7.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2 | Loss: 5.3886 | Acc: 0.0025 | F1: 0.0030\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/5: 100%|██████████| 50/50 [00:06<00:00,  7.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3 | Loss: 5.3066 | Acc: 0.0100 | F1: 0.0076\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/5: 100%|██████████| 50/50 [00:06<00:00,  7.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4 | Loss: 5.2340 | Acc: 0.0050 | F1: 0.0051\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/5: 100%|██████████| 50/50 [00:07<00:00,  7.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5 | Loss: 5.1599 | Acc: 0.0100 | F1: 0.0121\n","Saved model to: /content/drive/MyDrive/AML-PROJECT/pretrained_cnn_Ly/model2/resnet18__CrossEntropyLoss__SGD__20250705-144410.pth\n","\n","Running: LabelSmoothingLoss + Adam\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Epoch 1/5: 100%|██████████| 50/50 [00:07<00:00,  6.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 | Loss: 5.2713 | Acc: 0.0525 | F1: 0.0265\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/5: 100%|██████████| 50/50 [00:07<00:00,  7.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2 | Loss: 3.3961 | Acc: 0.3350 | F1: 0.2755\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/5: 100%|██████████| 50/50 [00:06<00:00,  7.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3 | Loss: 1.9540 | Acc: 0.3800 | F1: 0.3386\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/5: 100%|██████████| 50/50 [00:07<00:00,  7.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4 | Loss: 1.2961 | Acc: 0.7525 | F1: 0.7341\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/5: 100%|██████████| 50/50 [00:07<00:00,  7.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5 | Loss: 1.1060 | Acc: 0.8425 | F1: 0.8331\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Saved model to: /content/drive/MyDrive/AML-PROJECT/pretrained_cnn_Ly/model2/resnet18__LabelSmoothingLoss__Adam__20250705-144456.pth\n","\n","Running: LabelSmoothingLoss + SGD\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/5: 100%|██████████| 50/50 [00:06<00:00,  7.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 | Loss: 5.4755 | Acc: 0.0025 | F1: 0.0003\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/5: 100%|██████████| 50/50 [00:07<00:00,  7.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2 | Loss: 5.4003 | Acc: 0.0025 | F1: 0.0004\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/5: 100%|██████████| 50/50 [00:06<00:00,  7.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3 | Loss: 5.3282 | Acc: 0.0025 | F1: 0.0006\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/5: 100%|██████████| 50/50 [00:07<00:00,  7.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4 | Loss: 5.2633 | Acc: 0.0075 | F1: 0.0033\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/5: 100%|██████████| 50/50 [00:06<00:00,  7.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5 | Loss: 5.2025 | Acc: 0.0050 | F1: 0.0028\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Saved model to: /content/drive/MyDrive/AML-PROJECT/pretrained_cnn_Ly/model2/resnet18__LabelSmoothingLoss__SGD__20250705-144540.pth\n","\n","Running: FocalLoss + Adam\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/5: 100%|██████████| 50/50 [00:07<00:00,  6.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 | Loss: 5.2528 | Acc: 0.0525 | F1: 0.0292\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/5: 100%|██████████| 50/50 [00:07<00:00,  7.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2 | Loss: 3.0768 | Acc: 0.2125 | F1: 0.1656\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/5: 100%|██████████| 50/50 [00:07<00:00,  7.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3 | Loss: 1.1537 | Acc: 0.5275 | F1: 0.4739\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/5: 100%|██████████| 50/50 [00:07<00:00,  7.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4 | Loss: 0.3380 | Acc: 0.5650 | F1: 0.5147\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/5: 100%|██████████| 50/50 [00:07<00:00,  6.90it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5 | Loss: 0.1006 | Acc: 0.7200 | F1: 0.7032\n","Saved model to: /content/drive/MyDrive/AML-PROJECT/pretrained_cnn_Ly/model2/resnet18__FocalLoss__Adam__20250705-144625.pth\n","\n","Running: FocalLoss + SGD\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Epoch 1/5: 100%|██████████| 50/50 [00:07<00:00,  7.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 | Loss: 5.4389 | Acc: 0.0050 | F1: 0.0004\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/5: 100%|██████████| 50/50 [00:06<00:00,  7.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2 | Loss: 5.3467 | Acc: 0.0100 | F1: 0.0041\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/5: 100%|██████████| 50/50 [00:06<00:00,  7.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3 | Loss: 5.2525 | Acc: 0.0075 | F1: 0.0016\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/5: 100%|██████████| 50/50 [00:07<00:00,  7.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4 | Loss: 5.1721 | Acc: 0.0075 | F1: 0.0012\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/5: 100%|██████████| 50/50 [00:07<00:00,  7.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5 | Loss: 5.0912 | Acc: 0.0100 | F1: 0.0034\n","Saved model to: /content/drive/MyDrive/AML-PROJECT/pretrained_cnn_Ly/model2/resnet18__FocalLoss__SGD__20250705-144710.pth\n"]}]},{"cell_type":"code","source":["#  11. Summary Table\n","df_results = pd.DataFrame(results)\n","df_results = df_results.sort_values(by=\"accuracy\", ascending=False)\n","\n","print(\"\\n=== SUMMARY RESULTS ===\")\n","print(df_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0cmMuFj_Ft0l","executionInfo":{"status":"ok","timestamp":1751727310923,"user_tz":-480,"elapsed":54,"user":{"displayName":"liu muzi","userId":"17190698434235273658"}},"outputId":"f00ed67b-956e-4b8c-9097-0fac4f59671b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== SUMMARY RESULTS ===\n","              loss_fn optimizer  accuracy        f1  precision  recall  \\\n","2  LabelSmoothingLoss      Adam    0.8425  0.833089   0.868583  0.8425   \n","0    CrossEntropyLoss      Adam    0.8375  0.824595   0.855629  0.8375   \n","4           FocalLoss      Adam    0.7200  0.703169   0.773347  0.7200   \n","1    CrossEntropyLoss       SGD    0.0100  0.012143   0.019375  0.0100   \n","5           FocalLoss       SGD    0.0100  0.003413   0.002463  0.0100   \n","3  LabelSmoothingLoss       SGD    0.0050  0.002833   0.002167  0.0050   \n","\n","                    timestamp  \n","2  2025-07-05T14:44:56.237508  \n","0  2025-07-05T14:43:26.416919  \n","4  2025-07-05T14:46:25.978279  \n","1  2025-07-05T14:44:11.028072  \n","5  2025-07-05T14:47:10.877369  \n","3  2025-07-05T14:45:40.748115  \n"]}]},{"cell_type":"markdown","source":["## Resnet 18_1 Feature Extraction + svm/logistic on Noise Dataset"],"metadata":{"id":"3-ThBkuhKSKb"}},{"cell_type":"code","source":["save_dir = \"/content/drive/MyDrive/AML-PROJECT/pretrained_cnn_Ly/Resnet18_1_Noise\"\n","os.makedirs(save_dir, exist_ok=True)\n","\n","# 1. Data Loading\n","\n","class IrisDataset(Dataset):\n","    def __init__(self, df, transform=None):\n","        self.df = df\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.df.iloc[idx]['image_path']\n","        label = self.df.iloc[idx]['encoded_label']\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label\n","\n","# Data transformations (matching ResNet input requirements)\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","\n","train_dataset = IrisDataset(train_df, transform=transform)\n","val_dataset = IrisDataset(val_df, transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False) # shuffle=False to maintain feature order\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"],"metadata":{"id":"C53205rDLctU","executionInfo":{"status":"ok","timestamp":1751730866444,"user_tz":-480,"elapsed":8,"user":{"displayName":"liu muzi","userId":"17190698434235273658"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# 2.  CNN Feature Extraction (using pretrained ResNet18 without fine-tuning)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define ResNet18 feature extractor\n","class ResNet18FeatureExtractor(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.resnet = models.resnet18(pretrained=True)\n","        self.features = nn.Sequential(*list(self.resnet.children())[:-1]) # Remove final layer\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        return torch.flatten(x, 1)  # Flatten into feature vector\n","\n","# Global model initialization\n","model = ResNet18FeatureExtractor().to(device).eval()\n","\n","def extract_features(dataloader, model): # Accept model as parameter\n","    features, labels = [], []\n","    with torch.no_grad():\n","        for images, lbls in tqdm(dataloader,desc=\"Extracting Features\"):\n","            images = images.to(device)\n","            feats = model(images).cpu().numpy()\n","            features.append(feats)\n","            labels.append(lbls.numpy())\n","    return np.vstack(features), np.concatenate(labels)\n","\n","print(\"Extracting train features...\")\n","train_features, train_labels = extract_features(train_loader, model)\n","print(\"Extracting test features...\")\n","val_features, val_labels = extract_features(val_loader, model)\n","\n","# Save extracted features\n","np.save(os.path.join(save_dir, 'train_features.npy'), train_features)\n","np.save(os.path.join(save_dir, 'train_labels.npy'), train_labels)\n","np.save(os.path.join(save_dir, 'val_features.npy'), val_features)\n","np.save(os.path.join(save_dir, 'val_labels.npy'), val_labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qygCS5gFLqp1","executionInfo":{"status":"ok","timestamp":1751727363560,"user_tz":-480,"elapsed":8452,"user":{"displayName":"liu muzi","userId":"17190698434235273658"}},"outputId":"4092656f-e043-41cd-8842-439a56b979db"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Extracting train features...\n"]},{"output_type":"stream","name":"stderr","text":["Extracting Features: 100%|██████████| 50/50 [00:06<00:00,  7.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting test features...\n"]},{"output_type":"stream","name":"stderr","text":["Extracting Features: 100%|██████████| 13/13 [00:01<00:00,  8.14it/s]\n"]}]},{"cell_type":"code","source":["# 3. Train Classifiers (SVM and Logistic Regression)\n","# Feature standardization\n","scaler = StandardScaler()\n","train_features_scaled = scaler.fit_transform(train_features)\n","val_features_scaled = scaler.transform(val_features)\n","\n","# Save the scaler\n","joblib.dump(scaler, os.path.join(save_dir, 'feature_scaler.pkl'))\n","\n","# Initialize classifiers\n","svm = SVC(C=10, kernel='rbf', gamma='scale', probability=True)\n","lr = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n","\n","print(\"\\nTraining classifiers...\")\n","svm.fit(train_features_scaled, train_labels)\n","lr.fit(train_features_scaled, train_labels)\n","\n","# Save trained models\n","joblib.dump(svm, os.path.join(save_dir, 'svm_classifier.pkl'))\n","joblib.dump(lr, os.path.join(save_dir, 'logistic_regression_classifier.pkl'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HlOQw65FLyYH","executionInfo":{"status":"ok","timestamp":1751727380455,"user_tz":-480,"elapsed":14035,"user":{"displayName":"liu muzi","userId":"17190698434235273658"}},"outputId":"a1f4ed56-df2f-4c48-8242-e04c556f81a3"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training classifiers...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["['/content/drive/MyDrive/AML-PROJECT/pretrained_cnn_Ly/Resnet18_1_Noise/logistic_regression_classifier.pkl']"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# 4. Evaluation Result\n","\n","def evaluate_model(model, X_val, y_val, model_name):\n","    y_pred = model.predict(X_val)\n","    return {\n","        'variant': model_name,\n","        'accuracy': accuracy_score(y_val, y_pred),\n","        'f1': f1_score(y_val, y_pred, average='weighted', zero_division=0),\n","        'precision': precision_score(y_val, y_pred, average='weighted', zero_division=0),\n","        'recall': recall_score(y_val, y_pred, average='weighted', zero_division=0)\n","    }\n","\n","results = [\n","    evaluate_model(lr, val_features_scaled, val_labels, \"Logistic\"),\n","    evaluate_model(svm, val_features_scaled, val_labels, \"SVM_rbf\")\n","]\n","\n","# Format output\n","df_results = pd.DataFrame(results)\n","print(\"\\n=== SUMMARY RESULTS (ResNet18 Features + ML Classifiers) ===\")\n","print(df_results.to_string(\n","    index=False,\n","    columns=['variant', 'accuracy', 'f1', 'precision', 'recall'],\n","    float_format=\"%.4f\"\n","))\n","\n","torch.save(model.state_dict(), os.path.join(save_dir, 'resnet18_feature_extractor.pth'))\n","print(f\"\\nAll models and results saved to: {save_dir}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xd8CpDlFKq5B","executionInfo":{"status":"ok","timestamp":1751727521829,"user_tz":-480,"elapsed":701,"user":{"displayName":"liu muzi","userId":"17190698434235273658"}},"outputId":"d2de5ee4-1fa9-442e-dc74-464f98f30f37"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== SUMMARY RESULTS (ResNet18 Features + ML Classifiers) ===\n"," variant  accuracy     f1  precision  recall\n","Logistic    0.4650 0.4413     0.4749  0.4650\n"," SVM_rbf    0.4050 0.3808     0.4316  0.4050\n","\n","All models and results saved to: /content/drive/MyDrive/AML-PROJECT/pretrained_cnn_Ly/Resnet18_1_Noise\n"]}]},{"cell_type":"markdown","source":["## Resent18_2- Fine Tuning + Feature Extraction + SVM/LogReg on Noise Dataset"],"metadata":{"id":"5g7mrU0Z5iSI"}},{"cell_type":"code","source":["save_dir = \"/content/drive/MyDrive/AML-PROJECT/pretrained_cnn_Ly/Resnet18_2_Noise\"\n","os.makedirs(save_dir, exist_ok=True)\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\">>> Device:\", device)\n","\n","# 1. Data Loading\n","CSV_PATH = '/content/drive/MyDrive/AML-PROJECT/623final_all.csv'\n","BATCH_SIZE = 32\n","NUM_WORKERS = 2\n","\n","df = pd.read_csv(CSV_PATH)\n","df['ID'] = df['label'].str.extract(r'(\\d+)', expand=False)\n","train_df, val_df = train_test_split(\n","    df, test_size=0.2, stratify=df['ID'], random_state=42)\n","\n","le = LabelEncoder()\n","train_df['encoded_label'] = le.fit_transform(train_df['label'])\n","val_df['encoded_label']   = le.transform(val_df['label'])\n","num_classes = len(le.classes_)\n","print(f\">>> Num classes: {num_classes}\")\n","\n","class IrisDataset(Dataset):\n","    def __init__(self, df, tfm):\n","        self.df, self.tfm = df.reset_index(drop=True), tfm\n","    def __len__(self): return len(self.df)\n","    def __getitem__(self, idx):\n","        img = Image.open(self.df.loc[idx,'image_path']).convert('RGB')\n","        return self.tfm(img), self.df.loc[idx,'encoded_label']\n","\n","tfm = T.Compose([\n","    T.Resize((224,224)),\n","    T.ToTensor(),\n","    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","])\n","\n","train_loader = DataLoader(IrisDataset(train_df,tfm), batch_size=BATCH_SIZE,\n","                          shuffle=True, num_workers=NUM_WORKERS)\n","val_loader   = DataLoader(IrisDataset(val_df,tfm), batch_size=BATCH_SIZE,\n","                          shuffle=False, num_workers=NUM_WORKERS)\n","\n","\n","# 2. Custom Losses\n","class LabelSmoothingLoss(nn.Module):\n","    def __init__(self, classes, smoothing=0.1):\n","        super().__init__()\n","        self.confidence = 1.0 - smoothing\n","        self.smoothing  = smoothing\n","        self.classes    = classes\n","    def forward(self, pred, target):\n","        pred = pred.log_softmax(dim=-1)\n","        with torch.no_grad():\n","            true_dist = torch.zeros_like(pred)\n","            true_dist.fill_(self.smoothing / (self.classes-1))\n","            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n","        return torch.mean(torch.sum(-true_dist*pred, dim=-1))\n","\n","class FocalLoss(nn.Module):\n","    def __init__(self, gamma=2.0):\n","        super().__init__()\n","        self.gamma = gamma\n","        self.ce = nn.CrossEntropyLoss(reduction='none')\n","    def forward(self, pred, target):\n","        ce = self.ce(pred, target)\n","        pt = torch.exp(-ce)\n","        return ((1-pt)**self.gamma * ce).mean()\n","\n","# 3. Loss/Optimizer Combinations\n","\n","loss_fns = [\n","    ('CE', nn.CrossEntropyLoss()),\n","    ('LS', LabelSmoothingLoss(num_classes)),\n","    ('FL', FocalLoss(gamma=2))\n","]\n","opt_fns = [\n","    ('Adam', lambda p: optim.Adam(p, lr=3e-4, weight_decay=1e-4)),\n","    ('SGD' , lambda p: optim.SGD(p, lr=0.01, momentum=0.9, weight_decay=1e-4))\n","]\n","EPOCHS = 5\n","\n","\n","# 4. Encoder for ResNet18\n","\n","def make_encoder(model):\n","    return nn.Sequential(\n","        model.conv1, model.bn1, model.relu, model.maxpool,\n","        model.layer1, model.layer2, model.layer3, model.layer4,\n","        model.avgpool, nn.Flatten()\n","    ).eval().to(device)\n","\n","def extract(loader, enc):\n","    feats, lbls = [], []\n","    with torch.no_grad():\n","        for x,y in loader:\n","            feats.append(enc(x.to(device)).cpu().numpy())\n","            lbls.append(y.numpy())\n","    return np.vstack(feats), np.concatenate(lbls)\n","\n","# 5. Training + Feature Extraction + Evaluation + Model Saving\n","results = []\n","for lname, loss_fn in loss_fns:\n","    for oname, opt_fn in opt_fns:\n","        print(f\"\\n=== {lname} + {oname} ===\")\n","\n","        # Create subdirectory for current configuration\n","        config_dir = os.path.join(save_dir, f\"{lname}_{oname}\")\n","        os.makedirs(config_dir, exist_ok=True)\n","\n","        # Model initialization\n","        model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n","        model.fc = nn.Linear(model.fc.in_features, num_classes)\n","        for n,p in model.named_parameters():\n","            p.requires_grad = n.startswith('layer4') or n.startswith('fc')\n","        model = model.to(device)\n","        optimizer = opt_fn(filter(lambda p: p.requires_grad, model.parameters()))\n","\n","        # Training loop\n","        for ep in range(1, EPOCHS+1):\n","            model.train(); tot, corr, total = 0,0,0\n","            for x,y in train_loader:\n","                x,y = x.to(device), y.to(device)\n","                optimizer.zero_grad()\n","                out = model(x)\n","                loss = loss_fn(out, y)\n","                loss.backward(); optimizer.step()\n","                tot += loss.item()\n","                corr += (out.argmax(1)==y).sum().item(); total += y.size(0)\n","            print(f\"  Epoch {ep}/{EPOCHS} | TrainAcc {(corr/total):.4f}\")\n","\n","        # Save complete model and trained weights\n","        torch.save({\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': loss_fn.state_dict() if hasattr(loss_fn, 'state_dict') else None,\n","            'label_encoder': le.classes_  # Save label encoding info\n","        }, os.path.join(config_dir, 'full_model.pth'))\n","\n","        # Feature extraction and normalization\n","        enc = make_encoder(model)\n","        X_tr,y_tr = extract(train_loader, enc)\n","        X_va,y_va = extract(val_loader,   enc)\n","        norm = Normalizer('l2')\n","        X_tr = norm.fit_transform(X_tr); X_va = norm.transform(X_va)\n","\n","        # Save feature extractor and normalizer\n","        torch.save(enc.state_dict(), os.path.join(config_dir, 'feature_extractor.pth'))\n","        joblib.dump(norm, os.path.join(config_dir, 'normalizer.joblib'))\n","\n","        # Train and save classifiers\n","        svc = SVC(C=10, kernel='rbf', gamma='scale', probability=True).fit(X_tr,y_tr)\n","        logreg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000).fit(X_tr,y_tr)\n","\n","        joblib.dump(svc, os.path.join(config_dir, 'svc_classifier.joblib'))\n","        joblib.dump(logreg, os.path.join(config_dir, 'logreg_classifier.joblib'))\n","\n","        # Evaluation\n","        def evaluate(model, name):\n","            pred = model.predict(X_va)\n","            return {\n","                'Model'    : f\"ResNet18 + {name}\",\n","                'Loss'     : lname,\n","                'Opt'      : oname,\n","                'Accuracy' : accuracy_score(y_va, pred),\n","                'F1'       : f1_score(y_va, pred, average='weighted', zero_division=0),\n","                'Precision': precision_score(y_va, pred, average='weighted', zero_division=0),\n","                'Recall'   : recall_score(y_va, pred, average='weighted', zero_division=0)\n","            }\n","\n","        results.append(evaluate(svc, \"SVC\"))\n","        results.append(evaluate(logreg, \"LogReg\"))\n","\n","        print(f\"Model and components saved to: {config_dir}\")\n","\n","# 6. Save final results and configuration\n","df_results = pd.DataFrame(results)\n","df_results.to_csv(os.path.join(save_dir, 'all_results.csv'), index=False)\n","\n","# Save data preprocessing info\n","preprocess_info = {\n","    'image_size': 224,\n","    'normalization_mean': [0.485, 0.456, 0.406],\n","    'normalization_std': [0.229, 0.224, 0.225],\n","    'label_encoder': le.classes_\n","}\n","joblib.dump(preprocess_info, os.path.join(save_dir, 'preprocess_info.joblib'))\n","\n","print(\"\\n=== SUMMARY RESULTS ===\")\n","print(df_results.sort_values('Accuracy', ascending=False).to_string(index=False, float_format=\"%.4f\"))\n","print(f\"\\nAll models and results saved to: {save_dir}\")"],"metadata":{"id":"1i_09Jih5mpK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751728468728,"user_tz":-480,"elapsed":210170,"user":{"displayName":"liu muzi","userId":"17190698434235273658"}},"outputId":"1b4e258c-7afb-430f-902e-b58eff4ceb45"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":[">>> Device: cuda\n",">>> Num classes: 200\n","\n","=== CE + Adam ===\n","  Epoch 1/5 | TrainAcc 0.0919\n","  Epoch 2/5 | TrainAcc 0.8356\n","  Epoch 3/5 | TrainAcc 0.9950\n","  Epoch 4/5 | TrainAcc 1.0000\n","  Epoch 5/5 | TrainAcc 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Model and components saved to: /content/drive/MyDrive/AML-PROJECT/pretrained_cnn_Ly/Resnet18_2_Noise/CE_Adam\n","\n","=== CE + SGD ===\n","  Epoch 1/5 | TrainAcc 0.0319\n","  Epoch 2/5 | TrainAcc 0.5031\n","  Epoch 3/5 | TrainAcc 0.9406\n","  Epoch 4/5 | TrainAcc 0.9994\n","  Epoch 5/5 | TrainAcc 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Model and components saved to: /content/drive/MyDrive/AML-PROJECT/pretrained_cnn_Ly/Resnet18_2_Noise/CE_SGD\n","\n","=== LS + Adam ===\n","  Epoch 1/5 | TrainAcc 0.0956\n","  Epoch 2/5 | TrainAcc 0.8688\n","  Epoch 3/5 | TrainAcc 0.9981\n","  Epoch 4/5 | TrainAcc 1.0000\n","  Epoch 5/5 | TrainAcc 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Model and components saved to: /content/drive/MyDrive/AML-PROJECT/pretrained_cnn_Ly/Resnet18_2_Noise/LS_Adam\n","\n","=== LS + SGD ===\n","  Epoch 1/5 | TrainAcc 0.0369\n","  Epoch 2/5 | TrainAcc 0.5200\n","  Epoch 3/5 | TrainAcc 0.9363\n","  Epoch 4/5 | TrainAcc 0.9975\n","  Epoch 5/5 | TrainAcc 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Model and components saved to: /content/drive/MyDrive/AML-PROJECT/pretrained_cnn_Ly/Resnet18_2_Noise/LS_SGD\n","\n","=== FL + Adam ===\n","  Epoch 1/5 | TrainAcc 0.0887\n","  Epoch 2/5 | TrainAcc 0.8144\n","  Epoch 3/5 | TrainAcc 0.9931\n","  Epoch 4/5 | TrainAcc 1.0000\n","  Epoch 5/5 | TrainAcc 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Model and components saved to: /content/drive/MyDrive/AML-PROJECT/pretrained_cnn_Ly/Resnet18_2_Noise/FL_Adam\n","\n","=== FL + SGD ===\n","  Epoch 1/5 | TrainAcc 0.0306\n","  Epoch 2/5 | TrainAcc 0.5106\n","  Epoch 3/5 | TrainAcc 0.9375\n","  Epoch 4/5 | TrainAcc 0.9988\n","  Epoch 5/5 | TrainAcc 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Model and components saved to: /content/drive/MyDrive/AML-PROJECT/pretrained_cnn_Ly/Resnet18_2_Noise/FL_SGD\n","\n","=== SUMMARY RESULTS ===\n","            Model Loss  Opt  Accuracy     F1  Precision  Recall\n","   ResNet18 + SVC   CE Adam    0.7350 0.7214     0.7729  0.7350\n","   ResNet18 + SVC   FL Adam    0.7325 0.7293     0.8098  0.7325\n","   ResNet18 + SVC   LS Adam    0.7200 0.7102     0.7637  0.7200\n","   ResNet18 + SVC   CE  SGD    0.6875 0.6734     0.7219  0.6875\n","   ResNet18 + SVC   FL  SGD    0.6875 0.6703     0.7249  0.6875\n","   ResNet18 + SVC   LS  SGD    0.6825 0.6740     0.7454  0.6825\n","ResNet18 + LogReg   CE Adam    0.6475 0.6433     0.7429  0.6475\n","ResNet18 + LogReg   FL Adam    0.5975 0.5854     0.6881  0.5975\n","ResNet18 + LogReg   LS Adam    0.5900 0.5818     0.6760  0.5900\n","ResNet18 + LogReg   LS  SGD    0.5200 0.5133     0.6268  0.5200\n","ResNet18 + LogReg   CE  SGD    0.5075 0.4939     0.5860  0.5075\n","ResNet18 + LogReg   FL  SGD    0.4925 0.4726     0.5630  0.4925\n","\n","All models and results saved to: /content/drive/MyDrive/AML-PROJECT/pretrained_cnn_Ly/Resnet18_2_Noise\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"rlerLK0b5lRB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##  EfficientNet_1 Feature Extraction +SVM/ LogReg on Noise Dataset"],"metadata":{"id":"qlxQI2yLqtq5"}},{"cell_type":"code","source":["# === Set up save directory ===\n","save_dir = \"/content/drive/MyDrive/AML-PROJECT/pretrained_cnn_Ly/EfficientNet_1_Noise\"\n","os.makedirs(save_dir, exist_ok=True)\n","print(f\">>> Models will be saved to: {save_dir}\")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\">>> Using device:\", device)\n","\n","# === Paths and parameters ===\n","CSV_PATH     = '/content/drive/MyDrive/AML-PROJECT/623final_all.csv'\n","BATCH_SIZE   = 32\n","NUM_WORKERS  = 2   # Change to 0 on Windows\n","\n","# === Step 1: Data loading & label processing ===\n","df = pd.read_csv(CSV_PATH)\n","df['ID'] = df['label'].str.extract(r'(\\d+)', expand=False)\n","\n","train_df, val_df = train_test_split(\n","    df, test_size=0.2, stratify=df['ID'], random_state=42\n",")\n","\n","le = LabelEncoder()\n","train_df['encoded_label'] = le.fit_transform(train_df['label'])\n","val_df['encoded_label']   = le.transform(val_df['label'])\n","print(f\">>> Num classes: {len(le.classes_)}\")\n","\n","# Save label encoder\n","joblib.dump(le, os.path.join(save_dir, 'label_encoder.joblib'))\n","\n","# === Step 2: Dataset & DataLoader ===\n","class IrisDataset(Dataset):\n","    def __init__(self, df, tfm):\n","        self.df = df.reset_index(drop=True)\n","        self.tfm = tfm\n","\n","    def __len__(self): return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        img = Image.open(self.df.loc[idx, 'image_path']).convert('RGB')\n","        img = self.tfm(img)\n","        label = self.df.loc[idx, 'encoded_label']\n","        return img, label\n","\n","transform = T.Compose([\n","    T.Resize((224, 224)),  # EfficientNet B0 default input size\n","    T.ToTensor(),\n","    T.Normalize([0.485, 0.456, 0.406],\n","                [0.229, 0.224, 0.225])\n","])\n","\n","# Save transform info\n","joblib.dump({\n","    'image_size': 224,\n","    'normalization_mean': [0.485, 0.456, 0.406],\n","    'normalization_std': [0.229, 0.224, 0.225]\n","}, os.path.join(save_dir, 'transform_info.joblib'))\n","\n","train_loader = DataLoader(IrisDataset(train_df, transform),\n","                          batch_size=BATCH_SIZE, shuffle=False,\n","                          num_workers=NUM_WORKERS)\n","val_loader   = DataLoader(IrisDataset(val_df,   transform),\n","                          batch_size=BATCH_SIZE, shuffle=False,\n","                          num_workers=NUM_WORKERS)\n","\n","# === Step 3: EfficientNet-B0 Feature Extractor ===\n","class EffNetB0Extractor(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        model = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n","        self.features = nn.Sequential(model.features, model.avgpool)  # Output (B, 1280, 1, 1)\n","        for p in self.features.parameters():\n","            p.requires_grad = False\n","        self.features.eval()\n","\n","    def forward(self, x):\n","        with torch.no_grad():\n","            x = self.features(x)\n","            x = torch.flatten(x, 1)\n","        return x\n","\n","feature_extractor = EffNetB0Extractor().to(device)\n","\n","# Save feature extractor\n","torch.save(feature_extractor.state_dict(), os.path.join(save_dir, 'efficientnet_b0_extractor.pth'))\n","\n","def extract_features(loader):\n","    feats, labels = [], []\n","    for imgs, lbls in tqdm(loader, desc=\"► Extracting features\"):\n","        imgs = imgs.to(device)\n","        emb  = feature_extractor(imgs).cpu().numpy()\n","        feats.append(emb)\n","        labels.append(lbls.numpy())\n","    return np.vstack(feats), np.concatenate(labels)\n","\n","print(\"\\n[Stage] Feature extraction...\")\n","X_train_raw, y_train = extract_features(train_loader)\n","X_val_raw,   y_val   = extract_features(val_loader)\n","\n","# Save extracted features\n","np.save(os.path.join(save_dir, 'train_features.npy'), X_train_raw)\n","np.save(os.path.join(save_dir, 'val_features.npy'), X_val_raw)\n","np.save(os.path.join(save_dir, 'train_labels.npy'), y_train)\n","np.save(os.path.join(save_dir, 'val_labels.npy'), y_val)\n","\n","# === Step 4: L2 Normalization + Train Classifiers ===\n","l2 = Normalizer('l2')\n","X_train = l2.fit_transform(X_train_raw)\n","X_val   = l2.transform(X_val_raw)\n","\n","# Save normalizer\n","joblib.dump(l2, os.path.join(save_dir, 'l2_normalizer.joblib'))\n","\n","# Train and save classifiers\n","svc_rbf = SVC(kernel='rbf', C=10, gamma='scale')\n","svc_rbf.fit(X_train, y_train)\n","joblib.dump(svc_rbf, os.path.join(save_dir, 'rbf_svc.joblib'))\n","\n","logreg = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n","                            max_iter=1000, C=10)\n","logreg.fit(X_train, y_train)\n","joblib.dump(logreg, os.path.join(save_dir, 'logistic_regression.joblib'))\n","\n","# === Step 5: Evaluation and Reporting ===\n","def evaluate(model, name):\n","    pred = model.predict(X_val)\n","    return {\n","        'Model'    : name,\n","        'Accuracy' : accuracy_score(y_val, pred),\n","        'F1'       : f1_score(y_val, pred, average='weighted', zero_division=0),\n","        'Precision': precision_score(y_val, pred, average='weighted', zero_division=0),\n","        'Recall'   : recall_score(y_val, pred, average='weighted', zero_division=0)\n","    }\n","\n","results = [\n","    evaluate(svc_rbf, \"EffB0 + RBF SVM\"),\n","    evaluate(logreg, \"EffB0 + Logistic\")\n","]\n","\n","df_results = pd.DataFrame(results)\n","print(\"\\n=== SUMMARY RESULTS ===\")\n","print(df_results.to_string(index=False, float_format=\"%.4f\"))\n","\n","# Save results\n","df_results.to_csv(os.path.join(save_dir, 'evaluation_results.csv'), index=False)\n","\n","print(f\"\\n>>> All models and results saved to: {save_dir}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eMcZqIoIFIsh","executionInfo":{"status":"ok","timestamp":1751728965540,"user_tz":-480,"elapsed":19965,"user":{"displayName":"liu muzi","userId":"17190698434235273658"}},"outputId":"fab15881-fade-4f3f-a0a9-c49c3da55744"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":[">>> Models will be saved to: /content/drive/MyDrive/AML-PROJECT/pretrained_cnn_Ly/EfficientNet_1_Noise\n",">>> Using device: cuda\n",">>> Num classes: 200\n","\n","[Stage] Feature extraction...\n"]},{"output_type":"stream","name":"stderr","text":["► Extracting features: 100%|██████████| 50/50 [00:03<00:00, 12.78it/s]\n","► Extracting features: 100%|██████████| 13/13 [00:01<00:00, 11.92it/s]\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","=== SUMMARY RESULTS ===\n","           Model  Accuracy     F1  Precision  Recall\n"," EffB0 + RBF SVM    0.4750 0.4623     0.5292  0.4750\n","EffB0 + Logistic    0.4325 0.4157     0.4970  0.4325\n","\n",">>> All models and results saved to: /content/drive/MyDrive/AML-PROJECT/pretrained_cnn_Ly/EfficientNet_1_Noise\n"]}]},{"cell_type":"markdown","source":["## EfficientNet_2 Feature Extraction +  Fine Tuning + SVM/LogReg on Noise Dataset"],"metadata":{"id":"umVemmbm-awy"}},{"cell_type":"code","source":["import datetime\n","\n","save_dir = \"/content/drive/MyDrive/AML-PROJECT/pretrained_cnn_Ly/EfficientNet_2_Noise\"\n","os.makedirs(save_dir, exist_ok=True)\n","print(f\">>> All models will be saved to: {save_dir}\")\n","\n","\n","# EfficientNet-B0 Fine-tune + Feature Extraction + Linear Classifier Evaluation\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\">>> Device:\", device)\n","\n","# 1. Load CSV and process labels\n","\n","CSV_PATH = '/content/drive/MyDrive/AML-PROJECT/623final_all.csv'\n","BATCH_SIZE = 32\n","NUM_WORKERS = 2\n","\n","# Load and process DataFrame\n","df = pd.read_csv(CSV_PATH)\n","df['ID'] = df['label'].str.extract(r'(\\d+)', expand=False)\n","train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['ID'], random_state=42)\n","\n","# Encode labels\n","le = LabelEncoder()\n","train_df['encoded_label'] = le.fit_transform(train_df['label'])\n","val_df['encoded_label'] = le.transform(val_df['label'])\n","num_classes = len(le.classes_)\n","print(f\">>> Num classes: {num_classes}\")\n","\n","# Save label encoder\n","joblib.dump(le, os.path.join(save_dir, 'label_encoder.joblib'))\n","\n","# Define dataset\n","class IrisDataset(Dataset):\n","    def __init__(self, df, transform):\n","        self.df = df.reset_index(drop=True)\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        img = Image.open(self.df.loc[idx, 'image_path']).convert('RGB')\n","        return self.transform(img), self.df.loc[idx, 'encoded_label']\n","\n","# Define image transform\n","transform = T.Compose([\n","    T.Resize((224, 224)),\n","    T.ToTensor(),\n","    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","# Save transform info\n","joblib.dump({\n","    'image_size': 224,\n","    'normalization_mean': [0.485, 0.456, 0.406],\n","    'normalization_std': [0.229, 0.224, 0.225]\n","}, os.path.join(save_dir, 'transform_info.joblib'))\n","\n","# Loaders\n","train_loader = DataLoader(IrisDataset(train_df, transform), batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n","val_loader = DataLoader(IrisDataset(val_df, transform), batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n","\n","\n","# 2. Custom Loss Functions\n","\n","class LabelSmoothingLoss(nn.Module):\n","    def __init__(self, classes, smoothing=0.1):\n","        super().__init__()\n","        self.confidence = 1.0 - smoothing\n","        self.smoothing = smoothing\n","        self.classes = classes\n","\n","    def forward(self, pred, target):\n","        pred = pred.log_softmax(dim=-1)\n","        with torch.no_grad():\n","            true_dist = torch.zeros_like(pred)\n","            true_dist.fill_(self.smoothing / (self.classes - 1))\n","            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n","        return torch.mean(torch.sum(-true_dist * pred, dim=-1))\n","\n","class FocalLoss(nn.Module):\n","    def __init__(self, gamma=2.0):\n","        super().__init__()\n","        self.gamma = gamma\n","        self.ce = nn.CrossEntropyLoss(reduction='none')\n","\n","    def forward(self, pred, target):\n","        ce = self.ce(pred, target)\n","        pt = torch.exp(-ce)\n","        return ((1 - pt) ** self.gamma * ce).mean()\n","\n","# Loss and optimizer combinations\n","loss_fns = [\n","    ('CE', nn.CrossEntropyLoss()),\n","    ('LS', LabelSmoothingLoss(num_classes)),\n","    ('FL', FocalLoss(gamma=2))\n","]\n","opt_fns = [\n","    ('Adam', lambda p: optim.Adam(p, lr=3e-4, weight_decay=1e-4)),\n","    ('SGD', lambda p: optim.SGD(p, lr=0.01, momentum=0.9, weight_decay=1e-4))\n","]\n","\n","EPOCHS = 10\n","\n","\n","# 3. Feature Extractor & Evaluator & Saving\n","\n","def make_encoder(model):\n","    return nn.Sequential(model.features, model.avgpool, nn.Flatten()).eval().to(device)\n","\n","def extract_features(loader, encoder):\n","    feats, labels = [], []\n","    with torch.no_grad():\n","        for x, y in loader:\n","            feats.append(encoder(x.to(device)).cpu().numpy())\n","            labels.append(y.numpy())\n","    return np.vstack(feats), np.concatenate(labels)\n","\n","def evaluate(model, name, X, y):\n","    pred = model.predict(X)\n","    return {\n","        'Variant': name,\n","        'Accuracy': accuracy_score(y, pred),\n","        'F1': f1_score(y, pred, average='weighted', zero_division=0),\n","        'Precision': precision_score(y, pred, average='weighted', zero_division=0),\n","        'Recall': recall_score(y, pred, average='weighted', zero_division=0)\n","    }\n","\n","\n","# 4. Main Experiment Loop with Comprehensive Saving\n","\n","results = []\n","for lname, loss_fn in loss_fns:\n","    for oname, opt_fn in opt_fns:\n","        config_name = f\"{lname}_{oname}\"\n","        config_dir = os.path.join(save_dir, config_name)\n","        os.makedirs(config_dir, exist_ok=True)\n","        print(f\"\\n=== {config_name} ===\")\n","\n","        # Model initialization\n","        model = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n","        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n","\n","        # Freeze layers except last block and classifier\n","        for name, param in model.named_parameters():\n","            param.requires_grad = name.startswith('features.6') or name.startswith('classifier')\n","\n","        model = model.to(device)\n","        optimizer = opt_fn(filter(lambda p: p.requires_grad, model.parameters()))\n","\n","        # Training loop\n","        for epoch in range(1, EPOCHS + 1):\n","            model.train()\n","            total_loss, correct, total = 0, 0, 0\n","\n","            for x, y in train_loader:\n","                x, y = x.to(device), y.to(device)\n","                optimizer.zero_grad()\n","                out = model(x)\n","                loss = loss_fn(out, y)\n","                loss.backward()\n","                optimizer.step()\n","\n","                total_loss += loss.item()\n","                correct += (out.argmax(1) == y).sum().item()\n","                total += y.size(0)\n","\n","            print(f\"  Epoch {epoch}/{EPOCHS} | TrainAcc {(correct / total):.4f}\")\n","\n","        # Save complete model checkpoint\n","        torch.save({\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss_config': lname,\n","            'opt_config': oname,\n","            'num_classes': num_classes,\n","            'epoch': EPOCHS\n","        }, os.path.join(config_dir, 'full_checkpoint.pth'))\n","\n","        # Feature extraction\n","        encoder = make_encoder(model)\n","        X_train, y_train = extract_features(train_loader, encoder)\n","        X_val, y_val = extract_features(val_loader, encoder)\n","\n","        # Save feature extractor\n","        torch.save(encoder.state_dict(), os.path.join(config_dir, 'feature_extractor.pth'))\n","\n","        # Feature normalization\n","        norm = Normalizer('l2')\n","        X_train = norm.fit_transform(X_train)\n","        X_val = norm.transform(X_val)\n","        joblib.dump(norm, os.path.join(config_dir, 'feature_normalizer.joblib'))\n","\n","        # Train and save classifiers\n","        svm = SVC(C=10, kernel='rbf', gamma='scale', probability=True)\n","        svm.fit(X_train, y_train)\n","        joblib.dump(svm, os.path.join(config_dir, 'svm_classifier.joblib'))\n","\n","        lr = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n","        lr.fit(X_train, y_train)\n","        joblib.dump(lr, os.path.join(config_dir, 'logreg_classifier.joblib'))\n","\n","        # Evaluation\n","        svm_results = evaluate(svm, f\"{config_name} + rbfSVC\", X_val, y_val)\n","        lr_results = evaluate(lr, f\"{config_name} + Logistic\", X_val, y_val)\n","\n","        results.extend([svm_results, lr_results])\n","        print(f\"    [SVM] Acc {svm_results['Accuracy']:.4f} | F1 {svm_results['F1']:.4f}\")\n","        print(f\"    [LR] Acc {lr_results['Accuracy']:.4f} | F1 {lr_results['F1']:.4f}\")\n","\n","        # Save config-specific results\n","        pd.DataFrame([svm_results, lr_results]).to_csv(\n","            os.path.join(config_dir, 'config_results.csv'), index=False)\n","\n","# 5. Final Results and Experiment Summary\n","\n","# Save all results\n","df_results = pd.DataFrame(results).sort_values('Accuracy', ascending=False)\n","df_results.to_csv(os.path.join(save_dir, 'all_results.csv'), index=False)\n","\n","# Save experiment metadata\n","experiment_meta = {\n","    'timestamp': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n","    'batch_size': BATCH_SIZE,\n","    'num_workers': NUM_WORKERS,\n","    'epochs': EPOCHS,\n","    'image_size': 224,\n","    'num_classes': num_classes,\n","    'loss_functions': [l[0] for l in loss_fns],\n","    'optimizers': [o[0] for o in opt_fns],\n","    'best_accuracy': df_results['Accuracy'].max()\n","}\n","joblib.dump(experiment_meta, os.path.join(save_dir, 'experiment_metadata.joblib'))\n","\n","print(\"\\n=== FINAL RESULTS ===\")\n","print(df_results.to_string(index=False, float_format=\"%.4f\"))\n","print(f\"\\n>>> All components saved to: {save_dir}\")\n","print(f\">>> Best accuracy: {df_results['Accuracy'].max():.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"obTOXXlv-j5C","executionInfo":{"status":"ok","timestamp":1751731355390,"user_tz":-480,"elapsed":356983,"user":{"displayName":"liu muzi","userId":"17190698434235273658"}},"outputId":"74bbe42d-7e48-41a0-e545-d0365426255b"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":[">>> All models will be saved to: /content/drive/MyDrive/AML-PROJECT/pretrained_cnn_Ly/EfficientNet_2_Noise\n",">>> Device: cuda\n",">>> Num classes: 200\n","\n","=== CE_Adam ===\n","  Epoch 1/10 | TrainAcc 0.0112\n","  Epoch 2/10 | TrainAcc 0.2269\n","  Epoch 3/10 | TrainAcc 0.4537\n","  Epoch 4/10 | TrainAcc 0.6462\n","  Epoch 5/10 | TrainAcc 0.7900\n","  Epoch 6/10 | TrainAcc 0.8812\n","  Epoch 7/10 | TrainAcc 0.9369\n","  Epoch 8/10 | TrainAcc 0.9644\n","  Epoch 9/10 | TrainAcc 0.9850\n","  Epoch 10/10 | TrainAcc 0.9912\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["    [SVM] Acc 0.8000 | F1 0.7870\n","    [LR] Acc 0.7550 | F1 0.7491\n","\n","=== CE_SGD ===\n","  Epoch 1/10 | TrainAcc 0.0106\n","  Epoch 2/10 | TrainAcc 0.1681\n","  Epoch 3/10 | TrainAcc 0.3694\n","  Epoch 4/10 | TrainAcc 0.5281\n","  Epoch 5/10 | TrainAcc 0.6906\n","  Epoch 6/10 | TrainAcc 0.8087\n","  Epoch 7/10 | TrainAcc 0.8988\n","  Epoch 8/10 | TrainAcc 0.9425\n","  Epoch 9/10 | TrainAcc 0.9694\n","  Epoch 10/10 | TrainAcc 0.9781\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["    [SVM] Acc 0.7800 | F1 0.7714\n","    [LR] Acc 0.6775 | F1 0.6669\n","\n","=== LS_Adam ===\n","  Epoch 1/10 | TrainAcc 0.0163\n","  Epoch 2/10 | TrainAcc 0.2325\n","  Epoch 3/10 | TrainAcc 0.4863\n","  Epoch 4/10 | TrainAcc 0.6594\n","  Epoch 5/10 | TrainAcc 0.8056\n","  Epoch 6/10 | TrainAcc 0.9125\n","  Epoch 7/10 | TrainAcc 0.9444\n","  Epoch 8/10 | TrainAcc 0.9800\n","  Epoch 9/10 | TrainAcc 0.9869\n","  Epoch 10/10 | TrainAcc 0.9900\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["    [SVM] Acc 0.7925 | F1 0.7753\n","    [LR] Acc 0.7200 | F1 0.7138\n","\n","=== LS_SGD ===\n","  Epoch 1/10 | TrainAcc 0.0088\n","  Epoch 2/10 | TrainAcc 0.1644\n","  Epoch 3/10 | TrainAcc 0.3519\n","  Epoch 4/10 | TrainAcc 0.5112\n","  Epoch 5/10 | TrainAcc 0.6388\n","  Epoch 6/10 | TrainAcc 0.7662\n","  Epoch 7/10 | TrainAcc 0.8619\n","  Epoch 8/10 | TrainAcc 0.9206\n","  Epoch 9/10 | TrainAcc 0.9625\n","  Epoch 10/10 | TrainAcc 0.9731\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["    [SVM] Acc 0.7475 | F1 0.7361\n","    [LR] Acc 0.6275 | F1 0.6169\n","\n","=== FL_Adam ===\n","  Epoch 1/10 | TrainAcc 0.0187\n","  Epoch 2/10 | TrainAcc 0.2050\n","  Epoch 3/10 | TrainAcc 0.4575\n","  Epoch 4/10 | TrainAcc 0.6288\n","  Epoch 5/10 | TrainAcc 0.7825\n","  Epoch 6/10 | TrainAcc 0.9062\n","  Epoch 7/10 | TrainAcc 0.9481\n","  Epoch 8/10 | TrainAcc 0.9794\n","  Epoch 9/10 | TrainAcc 0.9856\n","  Epoch 10/10 | TrainAcc 0.9894\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["    [SVM] Acc 0.7800 | F1 0.7734\n","    [LR] Acc 0.7200 | F1 0.7109\n","\n","=== FL_SGD ===\n","  Epoch 1/10 | TrainAcc 0.0112\n","  Epoch 2/10 | TrainAcc 0.1644\n","  Epoch 3/10 | TrainAcc 0.3769\n","  Epoch 4/10 | TrainAcc 0.5375\n","  Epoch 5/10 | TrainAcc 0.7100\n","  Epoch 6/10 | TrainAcc 0.8569\n","  Epoch 7/10 | TrainAcc 0.9275\n","  Epoch 8/10 | TrainAcc 0.9706\n","  Epoch 9/10 | TrainAcc 0.9794\n","  Epoch 10/10 | TrainAcc 0.9812\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["    [SVM] Acc 0.7600 | F1 0.7520\n","    [LR] Acc 0.6775 | F1 0.6696\n","\n","=== FINAL RESULTS ===\n","           Variant  Accuracy     F1  Precision  Recall\n","  CE_Adam + rbfSVC    0.8000 0.7870     0.8221  0.8000\n","  LS_Adam + rbfSVC    0.7925 0.7753     0.8130  0.7925\n","  FL_Adam + rbfSVC    0.7800 0.7734     0.8380  0.7800\n","   CE_SGD + rbfSVC    0.7800 0.7714     0.8217  0.7800\n","   FL_SGD + rbfSVC    0.7600 0.7520     0.8073  0.7600\n","CE_Adam + Logistic    0.7550 0.7491     0.8193  0.7550\n","   LS_SGD + rbfSVC    0.7475 0.7361     0.7904  0.7475\n","FL_Adam + Logistic    0.7200 0.7109     0.7788  0.7200\n","LS_Adam + Logistic    0.7200 0.7138     0.7872  0.7200\n"," CE_SGD + Logistic    0.6775 0.6669     0.7466  0.6775\n"," FL_SGD + Logistic    0.6775 0.6696     0.7505  0.6775\n"," LS_SGD + Logistic    0.6275 0.6169     0.6940  0.6275\n","\n",">>> All components saved to: /content/drive/MyDrive/AML-PROJECT/pretrained_cnn_Ly/EfficientNet_2_Noise\n",">>> Best accuracy: 0.8000\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"AZaRIloPN9rx"},"execution_count":null,"outputs":[]}]}